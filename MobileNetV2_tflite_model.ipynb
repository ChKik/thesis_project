{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChKik/thesis_project/blob/development/MobileNetV2_tflite_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input #gia to -1,1 scaling twn pixel poy den eixa valei pio prin 22/7\n",
        "from tensorflow.keras import layers, models, regularizers, Input #l2 regularization gia to overfitting 22/7\n",
        "# gia tis katigories, tis kanei one hot encoding.\n",
        "# tha einai se ena binary numpy array me plithos column oses oi katigories\n",
        "import numpy as np\n",
        "import platform\n",
        "import pathlib\n",
        "import pandas as df\n",
        "from PIL import Image\n",
        "import os\n",
        "import random\n",
        "\n",
        "#checkarw ta versions poy xrisimopoiei to collab gia na apofugw conflicts.\n",
        "print('Python version:', platform.python_version())\n",
        "print(\"TF:\", tf.__version__)  #  version 2.18.0\n",
        "print(\"NumPy:\", np.__version__)  #  version 2.0.2\n",
        "print('Keras version:', tf.keras.__version__)"
      ],
      "metadata": {
        "id": "x-dEKlNVszF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive #only do it once at the start.\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "06zYytys04du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive_path= \"drive/My Drive/Diplomatiki/hand_gesture_dataset_resized.zip\"\n",
        "\n",
        "!unzip -q \"{drive_path}\" -d . #to kanei unzip kai to vazei sto current directory to file . Epeidh evala . ginetai ayto.\n"
      ],
      "metadata": {
        "id": "Khg8zh28LqHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_image(image): #kanei random flip,brightness ktlp tis fotografies kai tis allazei gia na kanoyme prevent to overfit. 22/7\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
        "    image = tf.image.random_contrast(image, 0.9, 1.1)\n",
        "    image = tf.image.random_saturation(image, 0.9, 1.1)\n",
        "    return image\n",
        "\n",
        "# Dataset path sto google drive\n",
        "base_path = \"/content/hand_gesture_dataset_resized\"\n",
        "\n",
        "#TRAINING FOR MobileNetV2 model which needs [-1,1] scaling for pixels (not 0-1 if using pretrained weights)\n",
        "def image_generator(subfolder, batch_size=32):\n",
        "    \"\"\"Generator that yields batches of images and labels on-demand\"\"\"\n",
        "    class_folders = [str(i) for i in range(20)]  # All class folders (0-19)\n",
        "\n",
        "    while True:  # Infinite loop for Keras compatibility\n",
        "        print(\"New Epoch\")\n",
        "        for class_label, class_name in enumerate(class_folders):\n",
        "            class_path = os.path.join(base_path, subfolder, class_name)\n",
        "\n",
        "            if not os.path.exists(class_path):\n",
        "                continue\n",
        "\n",
        "            batch_images = []\n",
        "            batch_labels = []\n",
        "            print(\"Class label \" + str(class_label)) # Convert class_label to string, thelei kai typecasting.\n",
        "            for img_name in os.listdir(class_path):\n",
        "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    img_path = os.path.join(class_path, img_name)\n",
        "                    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224)) #exei gisei hdh apo to script alla making sure\n",
        "\n",
        "                    img_array = tf.keras.preprocessing.image.img_to_array(img)  # load as array (range [0,255])\n",
        "                    img_array = tf.cast(img_array, tf.float32) #dokimazw na to kanw se float iws.\n",
        "                    img_array = augment_image(img_array) #Augment the jpg before preprocessing to try to prevent overfitting.Tha to testarw mono me ayto.\n",
        "                    img_array = preprocess_input(img_array)  #  scale to [-1,1] gia MobileNetV2\n",
        "                    batch_images.append(img_array)\n",
        "                    batch_labels.append(class_label)\n",
        "\n",
        "                    #stop otan exoyme arketa eimages, edw 32\n",
        "                    if len(batch_images) == batch_size:\n",
        "                        yield (np.array(batch_images),\n",
        "                               to_categorical(batch_labels, num_classes=20)) #onehot enc gia tis katigories kai einai np array. binary array me columns=classes num\n",
        "                        batch_images = []\n",
        "                        batch_labels = []\n",
        "\n",
        "            # Yield remaining images in partial batch\n",
        "            if batch_images:\n",
        "                yield (np.array(batch_images),\n",
        "                       to_categorical(batch_labels, num_classes=20))\n",
        "\n",
        "# Create generators instead of loading all data at once,fernei mexri 32 images kathe fora kai ekpaidevetai stadiaka.\n",
        "train_gen = image_generator(\"train\", batch_size=32)\n",
        "test_gen = image_generator(\"test\", batch_size=32)\n",
        "\n",
        "# Count total images (optional - for progress tracking)\n",
        "def count_images(subfolder):\n",
        "    count = 0\n",
        "    for class_label in range(20):\n",
        "        class_path = os.path.join(base_path, subfolder, str(class_label))\n",
        "        if os.path.exists(class_path):\n",
        "            count += len([f for f in os.listdir(class_path)\n",
        "                         if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "    return count\n",
        "\n",
        "train_count = count_images(\"train\")\n",
        "test_count = count_images(\"test\")\n",
        "print(f\"Total training images: {train_count}\")\n",
        "print(f\"Total test images: {test_count}\")\n",
        "\n",
        "# Create tf.data.Dataset from generators\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: train_gen,\n",
        "    output_types=(tf.float32, tf.float32),\n",
        "    output_shapes=((None, 224, 224, 3), (None, 20))\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: test_gen,\n",
        "    output_types=(tf.float32, tf.float32),\n",
        "    output_shapes=((None, 224, 224, 3), (None, 20))\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(\"\\nDatasets ready for training (RAM efficient)\")\n"
      ],
      "metadata": {
        "id": "1bo0vrNU8sEk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b7e3f2a-035c-4fe3-cce0-a5d9678965e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training images: 18000\n",
            "Total test images: 6000\n",
            "\n",
            "Datasets ready for training (RAM efficient)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.applications.MobileNetV2(\n",
        "    input_shape=(224, 224, 3),  # Must specify when include_top=True\n",
        "    alpha=0.5,  #back sto 0.35 to alpha, dokimazw to 0.5 23-7\n",
        "    include_top=False,  #Remove the original 1000-class layer\n",
        "    weights=\"imagenet\")\n",
        "\n",
        "#Custom head gia 20 classes mono\n",
        "# Input layer\n",
        "inputs = tf.keras.Input(shape=(224, 224, 3))  #layer0\n",
        "\n",
        "# Pass images through the base model\n",
        "x = model(inputs, training=False)  # Freeze to layer 1 gia na min xasw ta varh, layer1\n",
        "\n",
        "# Add new layers:\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)  # Reduces spatial dimensions apo 3d tis kanei 1d  , layer 2\n",
        "x = tf.keras.layers.Dropout(0.3)(x)              # Regularization to prevent overfitting, kanei merika layers drop, layer 3. Auksanw to dropout se 0.4->0.3 22/7\n",
        "outputs = tf.keras.layers.Dense(20, activation='softmax'      #,kernel_regularizer=regularizers.l2(0.01)  remove to l2 reg for now.\n",
        ")(x)  # 20-class prediction  , layer 4 , l2 regularization\n",
        "\n",
        "\n",
        "#Dokimzw kai early stop poy kanei monitor to val loss\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "model = tf.keras.Model(inputs, outputs, name=\"Zephyr_MobileNetV2\")\n"
      ],
      "metadata": {
        "id": "uWmgIaCn1GUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dokimzw kai early stop poy kanei monitor to val loss\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "#epeidh gia kapoio logo vazei CPU by default checkarei ama exeis valei gpu gia to runtime\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "\n",
        "model.layers[1].trainable = False  # Freeze MobileNetV2 (layer 1 stin periptwsh mas gia na min xalasei sto new training)\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#gia ta vimata twn epoch\n",
        "steps_per_epoch = train_count // 32\n",
        "validation_steps = test_count // 32\n",
        "\n",
        "#train to modelo edw.\n",
        "\n",
        "# 1. Freeze base and train head\n",
        "print(\"Training starts\")\n",
        "model.layers[1].trainable = False\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "model.fit( train_dataset,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=20,  #dokimazw na kanw increase ta epoch apo 10 se 20 mpas kai exw kalutera results sto training\n",
        "    validation_data=test_dataset,\n",
        "    validation_steps=validation_steps,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.TerminateOnNaN(),\n",
        "        tf.keras.callbacks.ModelCheckpoint('head_trained.keras') #to allaksa apo h5 se keras gia next , alla exw h5 twra.\n",
        "    ])  #evala kai vimata twra. Episis krataei ta kalutera weights se periptosi poy crasharei h to accuracy einai NaN\n",
        "\n",
        "# 2. Unfreeze kai to prwto layer and fine-tune to modelo\n",
        "print(\"Fine tuning starts\")\n",
        "model.layers[1].trainable = True\n",
        "model.compile(  optimizer=Adam(1e-5),  # Much lower learning rate\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy'])  # poly mikrotero learning apo prin\n",
        "\n",
        "print(\"Fine tuning begins\")\n",
        "model.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=10,\n",
        "    validation_data=test_dataset,\n",
        "    validation_steps=validation_steps,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.TerminateOnNaN(),\n",
        "        tf.keras.callbacks.ModelCheckpoint('fine_tuned.keras'), #to allaksa apo h5 se keras gia next , alla exw h5 twra.\n",
        "        early_stop\n",
        "    ])\n",
        "\n",
        "print(\"Training done \")"
      ],
      "metadata": {
        "id": "8PUMXz-W7qIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Quantization process now to convert all the float values into uint8 to reduce the size of inference.\n",
        "# ======================\n",
        "#  Post-training Quantization\n",
        "# Gia to teliko apotelesma tha exoyme inputs/outputs se range 0-255 uint8 gia na meiwsoyme to size anti gia float. Meiwsh apo 5.2mb se 658 kb etsi.\n",
        "# ======================\n",
        "\n",
        "def representative_dataset():\n",
        "    for images, _ in train_dataset.take(100):  # About 100 batches * 32 = ~3200 images\n",
        "        for i in range(images.shape[0]):\n",
        "            yield [images[i:i+1]]\n",
        "\n",
        "# Create TFLite converter from the trained Keras model\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "# Enable optimizations (this activates quantization)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Provide representative dataset for activation quantization\n",
        "converter.representative_dataset = representative_dataset\n",
        "\n",
        "# Set inference input/output types to uint8 for full integer quantization\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.uint8  # tha exei san input kai output to model uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "\n",
        "# Convert the model\n",
        "quantized_tflite_model = converter.convert()\n",
        "\n",
        "\n",
        "with open(\"zephyr_quantized_int8.tflite\", \"wb\") as f:\n",
        "    f.write(quantized_tflite_model)\n",
        "\n",
        "print(\"Quantized model saved as zephyr_quantized_int8.tflite\")\n"
      ],
      "metadata": {
        "id": "EhUOtn-4s_VR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VfeeY3IvPU6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This cell ensures that the model expects the input image of 0-255 quantized uint8 image I am gonna pass to it.\n",
        "interpreter = tf.lite.Interpreter(model_path=\"/content/drive/MyDrive/Diplomatiki/output/zephyr_quantized_int8.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "input_info = interpreter.get_input_details()[0]\n",
        "print(input_info['dtype'], input_info['quantization'])"
      ],
      "metadata": {
        "id": "xio-3Rzxk7xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "me99wk3hQokk"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1RpU_GQcSdzdeQ2AWnv0SVNYbLYB32K0-",
      "authorship_tag": "ABX9TyMNdoAPsTNqoOfhto6bIIhc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}